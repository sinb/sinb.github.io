<!DOCTYPE HTML>
<html>
<head>
<meta name="baidu-site-verification" content="xdZ6LobkoN" />
  <meta charset="utf-8">
  
  <title>用Theano做一个MLP | Amnesia&#39;s blog</title>
  <meta name="author" content="Amnesia">
  
  <meta name="description" content="前几天看Deep learning上的Theano教程,看到shared variable就不明白说的啥,暂时放弃.">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="用Theano做一个MLP"/>
  <meta property="og:site_name" content="Amnesia&#39;s blog"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Amnesia&#39;s blog" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-54965599-1']);
  
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>



</head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">Amnesia&#39;s blog</a></h1>
  <h2><a href="/">I&#39;ll be your mirror</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">主页</a></li>
    
      <li><a href="/categories/Life">生活</a></li>
    
      <li><a href="/categories/Whatyouknowabout">科普</a></li>
    
      <li><a href="/categories/Courseranote">Coursera笔记</a></li>
    
      <li><a href="/categories/Artworks">Artworks</a></li>
    
      <li><a href="/links">友链</a></li>
    
      <li><a href="/about">关于</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div></header>
    <script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
	
  <script src="https://raw.github.com/processing-js/processing-js/v1.4.8/processing.min.js" type="text/javascript"></script> 
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2014-12-26T12:39:42.000Z"><a href="/Whatyouknowabout/theano-mlp/">2014-12-26</a></time>
      
      
  
    <h1 class="title">用Theano做一个MLP</h1>
  

    </header>
    <div class="entry">
      
        <p>前几天看<a href="http://deeplearning.net/" target="_blank" rel="external">Deep learning</a>上的<a href="http://deeplearning.net/software/theano/tutorial/" target="_blank" rel="external">Theano教程</a>,看到shared variable就不明白说的啥,暂时放弃.<br><a id="more"></a><br>今天发现了网站上还有一个<a href="http://nbviewer.ipython.org/github/craffel/theano-tutorial/blob/master/Theano%20Tutorial.ipynb" target="_blank" rel="external">简化版教程</a>用theano写了一个MLP(multi-layer perceptron),这个好懂多了,所以做个介绍.教程实现的就是一个单隐层的神经网络,也就是一个MLP.</p>
<p>Theano里什么CUDA GPU计算、自动差分的就不说了.Theano里为什么有shared variable这个东西.Shared variable就像全局变量一样可以被不同的使用这些变量的函数更新(函数一执行,变量就按照规则去更新),而且这些变量还有状态.这个东西就是为了机器学习模型的参数而设计的,因为模型参数需要通过迭代一次次更新,这些参数都是shared variables.</p>
<p>下面这个就是MLP中一个层,这一层有一个连接权值W,偏置b,和一个激活函数activation(比如sigmoid之类),W和b都是shared variables,之后要更新它们.output函数返回的就是W<em>x+b,或者如果指定激活函数就返回activation(W</em>x+b).</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class Layer(object):&#10;    def __init__(self, W_init, b_init, activation):&#10;        &#39;&#39;&#39;&#10;        A layer of a neural network, computes s(Wx + b) where s is a nonlinearity and x is the input vector.&#10;&#10;        :parameters:&#10;            - W_init : np.ndarray, shape=(n_output, n_input)&#10;                Values to initialize the weight matrix to.&#10;            - b_init : np.ndarray, shape=(n_output,)&#10;                Values to initialize the bias vector&#10;            - activation : theano.tensor.elemwise.Elemwise&#10;                Activation function for layer output&#10;        &#39;&#39;&#39;&#10;        # Retrieve the input and output dimensionality based on W&#39;s initialization&#10;        n_output, n_input = W_init.shape&#10;        # Make sure b is n_output in size&#10;        assert b_init.shape == (n_output,)&#10;        # All parameters should be shared variables.&#10;        # They&#39;re used in this class to compute the layer output,&#10;        # but are updated elsewhere when optimizing the network parameters.&#10;        # Note that we are explicitly requiring that W_init has the theano.config.floatX dtype&#10;        self.W = theano.shared(value=W_init.astype(theano.config.floatX),&#10;                               # The name parameter is solely for printing purporses&#10;                               name=&#39;W&#39;,&#10;                               # Setting borrow=True allows Theano to use user memory for this object.&#10;                               # It can make code slightly faster by avoiding a deep copy on construction.&#10;                               # For more details, see&#10;                               # http://deeplearning.net/software/theano/tutorial/aliasing.html&#10;                               borrow=True)&#10;        # We can force our bias vector b to be a column vector using numpy&#39;s reshape method.&#10;        # When b is a column vector, we can pass a matrix-shaped input to the layer&#10;        # and get a matrix-shaped output, thanks to broadcasting (described below)&#10;        self.b = theano.shared(value=b_init.reshape(-1, 1).astype(theano.config.floatX),&#10;                               name=&#39;b&#39;,&#10;                               borrow=True,&#10;                               # Theano allows for broadcasting, similar to numpy.&#10;                               # However, you need to explicitly denote which axes can be broadcasted.&#10;                               # By setting broadcastable=(False, True), we are denoting that b&#10;                               # can be broadcast (copied) along its second dimension in order to be&#10;                               # added to another variable.  For more information, see&#10;                               # http://deeplearning.net/software/theano/library/tensor/basic.html&#10;                               broadcastable=(False, True))&#10;        self.activation = activation&#10;        # We&#39;ll compute the gradient of the cost of the network with respect to the parameters in this list.&#10;        self.params = [self.W, self.b]&#10;        &#10;    def output(self, x):&#10;        &#39;&#39;&#39;&#10;        Compute this layer&#39;s output given an input&#10;        &#10;        :parameters:&#10;            - x : theano.tensor.var.TensorVariable&#10;                Theano symbolic variable for layer input&#10;&#10;        :returns:&#10;            - output : theano.tensor.var.TensorVariable&#10;                Mixed, biased, and activated x&#10;        &#39;&#39;&#39;&#10;        # Compute linear mix&#10;        lin_output = T.dot(self.W, x) + self.b&#10;        # Output is just linear mix if no activation function&#10;        # Otherwise, apply the activation function&#10;        return (lin_output if self.activation is None else self.activation(lin_output))</span><br></pre></td></tr></table></figure>
<p>然后下面这就是一个MLP,由几个layer和几个函数组成.首先要根据输入的W,b,activation的维度确定这是个几层的MLP,然后对每层赋值(每层都是一个Layer).MLP的两个函数,一个output执行forward计算,一层一层计算输出,最后返回MLP的输出;另一个squared_root,根据output的返回值和数据标签y进行对比,也就是误差函数,一会更新参数时需要计算这个误差函数的梯度.<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class MLP(object):&#10;    def __init__(self, W_init, b_init, activations):&#10;        &#39;&#39;&#39;&#10;        Multi-layer perceptron class, computes the composition of a sequence of Layers&#10;&#10;        :parameters:&#10;            - W_init : list of np.ndarray, len=N&#10;                Values to initialize the weight matrix in each layer to.&#10;                The layer sizes will be inferred from the shape of each matrix in W_init&#10;            - b_init : list of np.ndarray, len=N&#10;                Values to initialize the bias vector in each layer to&#10;            - activations : list of theano.tensor.elemwise.Elemwise, len=N&#10;                Activation function for layer output for each layer&#10;        &#39;&#39;&#39;&#10;        # Make sure the input lists are all of the same length&#10;        assert len(W_init) == len(b_init) == len(activations)&#10;        &#10;        # Initialize lists of layers&#10;        self.layers = []&#10;        # Construct the layers&#10;        for W, b, activation in zip(W_init, b_init, activations):&#10;            self.layers.append(Layer(W, b, activation))&#10;&#10;        # Combine parameters from all layers&#10;        self.params = []&#10;        for layer in self.layers:&#10;            self.params += layer.params&#10;        &#10;    def output(self, x):&#10;        &#39;&#39;&#39;&#10;        Compute the MLP&#39;s output given an input&#10;        &#10;        :parameters:&#10;            - x : theano.tensor.var.TensorVariable&#10;                Theano symbolic variable for network input&#10;&#10;        :returns:&#10;            - output : theano.tensor.var.TensorVariable&#10;                x passed through the MLP&#10;        &#39;&#39;&#39;&#10;        # Recursively compute output&#10;        for layer in self.layers:&#10;            x = layer.output(x)&#10;        return x&#10;&#10;    def squared_error(self, x, y):&#10;        &#39;&#39;&#39;&#10;        Compute the squared euclidean error of the network output against the &#34;true&#34; output y&#10;        &#10;        :parameters:&#10;            - x : theano.tensor.var.TensorVariable&#10;                Theano symbolic variable for network input&#10;            - y : theano.tensor.var.TensorVariable&#10;                Theano symbolic variable for desired network output&#10;&#10;        :returns:&#10;            - error : theano.tensor.var.TensorVariable&#10;                The squared Euclidian distance between the network output and y&#10;        &#39;&#39;&#39;&#10;        return T.sum((self.output(x) - y)**2)</span><br></pre></td></tr></table></figure></p>
<p>MLP创建完,还有就是梯度下降了.一般要进行梯度下降就要先写出误差函数,然后写出误差函数对于参数的导数函数,根据函数计算梯度.除了这个,还可以用数值方法计算梯度.UFLDL上说,写出导数函数后,需要梯度检查,用数值的方法计算梯度,和推导出的函数计算出的结果进行对比,确保无误.用数值的方法计算梯度也就是直接取极限,用两个相差很小的x计算出的f(x),作差,求商,这样做很慢,所以UFLDL上提醒在随后进行训练的时候要记着关掉梯度检查.Theano用的方法比这两个都聪明,它的符号表达式组成了一个图,从这个图里可以找到求差分的方法,就能自动差分了,不用写梯度表达式也不用数值计算的方法.下面这个梯度下降使用了动量来控制参数更新.<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def gradient_updates_momentum(cost, params, learning_rate, momentum):&#10;    &#39;&#39;&#39;&#10;    Compute updates for gradient descent with momentum&#10;    &#10;    :parameters:&#10;        - cost : theano.tensor.var.TensorVariable&#10;            Theano cost function to minimize&#10;        - params : list of theano.tensor.var.TensorVariable&#10;            Parameters to compute gradient against&#10;        - learning_rate : float&#10;            Gradient descent learning rate&#10;        - momentum : float&#10;            Momentum parameter, should be at least 0 (standard gradient descent) and less than 1&#10;   &#10;    :returns:&#10;        updates : list&#10;            List of updates, one for each parameter&#10;    &#39;&#39;&#39;&#10;    # Make sure momentum is a sane value&#10;    assert momentum &#60; 1 and momentum &#62;= 0&#10;    # List of update steps for each parameter&#10;    updates = []&#10;    # Just gradient descent on cost&#10;    for param in params:&#10;        # For each parameter, we&#39;ll create a param_update shared variable.&#10;        # This variable will keep track of the parameter&#39;s update step across iterations.&#10;        # We initialize it to 0&#10;        param_update = theano.shared(param.get_value()*0., broadcastable=param.broadcastable)&#10;        # Each parameter is updated by taking a step in the direction of the gradient.&#10;        # However, we also &#34;mix in&#34; the previous step according to the given momentum value.&#10;        # Note that when updating param_update, we are using its old value and also the new gradient step.&#10;        updates.append((param, param - learning_rate*param_update))&#10;        # Note that we don&#39;t need to derive backpropagation to compute updates - just use T.grad!&#10;        updates.append((param_update, momentum*param_update + (1. - momentum)*T.grad(cost, param)))&#10;    return updates</span><br></pre></td></tr></table></figure></p>
<p>最后是一个实例,产生了两个二维高斯分布,一个中心点在[-1,-1], 一个在[1,1],方差给了随机值,总之就是两个分布.一共有1000个点在这两个分布里,使用MLP对这1000个点进行分类.训练数据X.shape = (2, 1000),标签y.shape = (1000,).<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># Training data - two randomly-generated Gaussian-distributed clouds of points in 2d space&#10;np.random.seed(0)&#10;# Number of points&#10;N = 1000&#10;# Labels for each cluster&#10;y = np.random.random_integers(0, 1, N)&#10;# Mean of each cluster&#10;means = np.array([[-1, 1], [-1, 1]])&#10;# Covariance (in X and Y direction) of each cluster&#10;covariances = np.random.random_sample((2, 2)) + 1&#10;# Dimensions of each point&#10;X = np.vstack([np.random.randn(N)*covariances[0, y] + means[0, y],&#10;               np.random.randn(N)*covariances[1, y] + means[1, y]])&#10;# Plot the data&#10;plt.figure(figsize=(8, 8))&#10;plt.scatter(X[0, :], X[1, :], c=y, lw=.3, s=3, cmap=plt.cm.cool)&#10;plt.axis([-6, 6, -6, 6])&#10;plt.show()</span><br></pre></td></tr></table></figure></p>
<p>之后要创建一个MLP,这里用了输入大小是2(二维),隐层大小4,输出大小1,每层都使用sigmoid函数.<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># First, set the size of each layer (and the number of layers)&#10;# Input layer size is training data dimensionality (2)&#10;# Output size is just 1-d: class label - 0 or 1&#10;# Finally, let the hidden layers be twice the size of the input.&#10;# If we wanted more layers, we could just add another layer size to this list.&#10;layer_sizes = [X.shape[0], X.shape[0]*2, 1]&#10;# Set initial parameter values&#10;W_init = []&#10;b_init = []&#10;activations = []&#10;for n_input, n_output in zip(layer_sizes[:-1], layer_sizes[1:]):&#10;    # Getting the correct initialization matters a lot for non-toy problems.&#10;    # However, here we can just use the following initialization with success:&#10;    # Normally distribute initial weights&#10;    W_init.append(np.random.randn(n_output, n_input))&#10;    # Set initial biases to 1&#10;    b_init.append(np.ones(n_output))&#10;    # We&#39;ll use sigmoid activation for all layers&#10;    # Note that this doesn&#39;t make a ton of sense when using squared distance&#10;    # because the sigmoid function is bounded on [0, 1].&#10;    activations.append(T.nnet.sigmoid)&#10;# Create an instance of the MLP class&#10;mlp = MLP(W_init, b_init, activations)&#10;&#10;# Create Theano variables for the MLP input&#10;mlp_input = T.matrix(&#39;mlp_input&#39;)&#10;# ... and the desired output&#10;mlp_target = T.vector(&#39;mlp_target&#39;)&#10;# Learning rate and momentum hyperparameter values&#10;# Again, for non-toy problems these values can make a big difference&#10;# as to whether the network (quickly) converges on a good local minimum.&#10;learning_rate = 0.01&#10;momentum = 0.9&#10;# Create a function for computing the cost of the network given an input&#10;cost = mlp.squared_error(mlp_input, mlp_target)&#10;# Create a theano function for training the network&#10;train = theano.function([mlp_input, mlp_target], cost,&#10;                        updates=gradient_updates_momentum(cost, mlp.params, learning_rate, momentum))&#10;# Create a theano function for computing the MLP&#39;s output given some input&#10;mlp_output = theano.function([mlp_input], mlp.output(mlp_input))</span><br></pre></td></tr></table></figure></p>
<p>然后就开始迭代,这里用了20次迭代.<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># Keep track of the number of training iterations performed&#10;iteration = 0&#10;# We&#39;ll only train the network with 20 iterations.&#10;# A more common technique is to use a hold-out validation set.&#10;# When the validation error starts to increase, the network is overfitting,&#10;# so we stop training the net.  This is called &#34;early stopping&#34;, which we won&#39;t do here.&#10;max_iteration = 20&#10;while iteration &#60; max_iteration:&#10;    # Train the network using the entire training set.&#10;    # With large datasets, it&#39;s much more common to use stochastic or mini-batch gradient descent&#10;    # where only a subset (or a single point) of the training set is used at each iteration.&#10;    # This can also help the network to avoid local minima.&#10;    current_cost = train(X, y)&#10;    # Get the current network output for all points in the training set&#10;    current_output = mlp_output(X)&#10;    # We can compute the accuracy by thresholding the output&#10;    # and computing the proportion of points whose class match the ground truth class.&#10;    accuracy = np.mean((current_output &#62; .5) == y)&#10;    # Plot network output after this iteration&#10;    plt.figure(figsize=(8, 8))&#10;    plt.scatter(X[0, :], X[1, :], c=current_output,&#10;                lw=.3, s=3, cmap=plt.cm.cool, vmin=0, vmax=1)&#10;    plt.axis([-6, 6, -6, 6])&#10;    plt.title(&#39;Cost: &#123;:.3f&#125;, Accuracy: &#123;:.3f&#125;&#39;.format(float(current_cost), accuracy))&#10;    plt.show()&#10;    iteration += 1</span><br></pre></td></tr></table></figure></p>

      
    </div>
    <footer>
      
        
  
  <div class="categories">
    <a href="/categories/Whatyouknowabout/">Whatyouknowabout</a>
  </div>

        
  
  <div class="tags">
    <a href="/tags/Theano/">Theano</a>
  </div>

        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
<!-- Duoshuo Comment BEGIN -->
	<div class="ds-thread"></div>
<script type="text/javascript">
var duoshuoQuery = {short_name:"sinb"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = 'http://static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		|| document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
<!-- Duoshuo Comment END -->
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="搜尋">
    <input type="hidden" name="q" value="site:sinb.github.io">
  </form>
</div>

  
<div class="widget tag">
  <h3 class="title">分類</h3>
  <ul class="entry">
  
    <li><a href="/categories/Artworks/">Artworks</a><small>6</small></li>
  
    <li><a href="/categories/Courseranote/">Courseranote</a><small>1</small></li>
  
    <li><a href="/categories/Life/">Life</a><small>5</small></li>
  
    <li><a href="/categories/Whatyouknowabout/">Whatyouknowabout</a><small>16</small></li>
  
  </ul>
</div>


  
<div class="widget tag">
  <h3 class="title">標籤</h3>
  <ul class="entry">
  
    <li><a href="/tags/Algorithms/">Algorithms</a><small>1</small></li>
  
    <li><a href="/tags/CNN/">CNN</a><small>2</small></li>
  
    <li><a href="/tags/CS-Foundation/">CS Foundation</a><small>1</small></li>
  
    <li><a href="/tags/Coursera/">Coursera</a><small>1</small></li>
  
    <li><a href="/tags/DBN/">DBN</a><small>1</small></li>
  
    <li><a href="/tags/Deep-Learning/">Deep Learning</a><small>2</small></li>
  
    <li><a href="/tags/Imagenet/">Imagenet</a><small>1</small></li>
  
    <li><a href="/tags/MNIST/">MNIST</a><small>1</small></li>
  
    <li><a href="/tags/PCA/">PCA</a><small>1</small></li>
  
    <li><a href="/tags/Paper/">Paper</a><small>3</small></li>
  
    <li><a href="/tags/Processing/">Processing</a><small>5</small></li>
  
    <li><a href="/tags/Theano/">Theano</a><small>1</small></li>
  
    <li><a href="/tags/UFLDL/">UFLDL</a><small>7</small></li>
  
    <li><a href="/tags/WeekEnd/">WeekEnd</a><small>1</small></li>
  
    <li><a href="/tags/fft/">fft</a><small>1</small></li>
  
    <li><a href="/tags/吉他/">吉他</a><small>1</small></li>
  
    <li><a href="/tags/喝酒/">喝酒</a><small>1</small></li>
  
    <li><a href="/tags/最小二乘/">最小二乘</a><small>1</small></li>
  
    <li><a href="/tags/最近/">最近</a><small>1</small></li>
  
    <li><a href="/tags/演出/">演出</a><small>1</small></li>
  
    <li><a href="/tags/电影/">电影</a><small>1</small></li>
  
  </ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2015 Amnesia
  
</div>
<div class="clearfix"></div></footer>
  <script src="http://ajax.useso.com/ajax/libs/jquery/1.7.2/jquery.min.js"></script>
<script src="/js/gallery.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>




</body>
</html>