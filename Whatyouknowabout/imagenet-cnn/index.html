<!DOCTYPE HTML>
<html>
<head>
<meta name="baidu-site-verification" content="xdZ6LobkoN" />
  <meta charset="utf-8">
  
  <title>[论文笔记] ImageNet Classification with Deep ConvolutionalNeural Networks | Amnesia&#39;s blog</title>
  <meta name="author" content="Amnesia">
  
  <meta name="description" content="cuda-convnet这篇文章提出的CNN模型在imagenet 2012比赛中得了冠军,之后这个CNN就用作者的名字叫做AlexNet了.Alex还写了一个CNN工具cuda-convnet用来跑它的模型,这个工具实用又相比caffe较轻量级.看cuda-convnet文档的时候看到一个有意思的,记录一下.">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="[论文笔记] ImageNet Classification with Deep ConvolutionalNeural Networks"/>
  <meta property="og:site_name" content="Amnesia&#39;s blog"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/css/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Amnesia&#39;s blog" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-54965599-1']);
  
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>



</head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">Amnesia&#39;s blog</a></h1>
  <h2><a href="/">I&#39;ll be your mirror</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">主页</a></li>
    
      <li><a href="/categories/Life">生活</a></li>
    
      <li><a href="/categories/Whatyouknowabout">科普</a></li>
    
      <li><a href="/categories/Courseranote">Coursera笔记</a></li>
    
      <li><a href="/categories/Artworks">Artworks</a></li>
    
      <li><a href="/links">友链</a></li>
    
      <li><a href="/about">关于</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div></header>
    <script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
	
  <script src="https://raw.github.com/processing-js/processing-js/v1.4.8/processing.min.js" type="text/javascript"></script> 
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2015-05-11T07:49:58.000Z"><a href="/Whatyouknowabout/imagenet-cnn/">2015-05-11</a></time>
      
      
  
    <h1 class="title">[论文笔记] ImageNet Classification with Deep ConvolutionalNeural Networks</h1>
  

    </header>
    <div class="entry">
      
        <h2 id="cuda-convnet">cuda-convnet</h2><p>这篇文章提出的CNN模型在imagenet 2012比赛中得了冠军,之后这个CNN就用作者的名字叫做AlexNet了.Alex还写了一个CNN工具cuda-convnet用来跑它的模型,这个工具实用又相比caffe较轻量级.看cuda-convnet文档的时候看到一个有意思的,记录一下.<br><a id="more"></a></p>
<h3 id="Locally-connected_layer_with_unshared_weights">Locally-connected layer with unshared weights</h3><p>This kind of layer is just like a convolutional layer, but without any weight-sharing. That is to say, a different set of filters is applied at every (x, y) location in the input image. Aside from that, it behaves exactly as a convolutional layer.</p>
<p>Here’s how to define such a layer, taking the conv32 layer as input:<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">[local32]</span></span><br><span class="line"><span class="setting">type=<span class="value">local</span></span></span><br><span class="line"><span class="setting">inputs=<span class="value">conv32</span></span></span><br><span class="line"><span class="setting">channels=<span class="value"><span class="number">32</span></span></span></span><br><span class="line"><span class="setting">filters=<span class="value"><span class="number">32</span></span></span></span><br><span class="line"><span class="setting">padding=<span class="value"><span class="number">4</span></span></span></span><br><span class="line"><span class="setting">stride=<span class="value"><span class="number">1</span></span></span></span><br><span class="line"><span class="setting">filterSize=<span class="value"><span class="number">9</span></span></span></span><br><span class="line"><span class="setting">neuron=<span class="value">logistic</span></span></span><br><span class="line"><span class="setting">initW=<span class="value"><span class="number">0.00001</span></span></span></span><br><span class="line"><span class="setting">initB=<span class="value"><span class="number">0</span></span></span></span><br></pre></td></tr></table></figure></p>
<p>Aside from the type=local line, there’s nothing new here. Note however that since there is no weight sharing, the actual number of distinct filters in this layer will be filters multiplied by however many filter applications are required to cover the entire image and padding with the given stride.<br>字面上来理解,以前CNN里的一个filter可以用在一整张图上,经过卷积得到一个feature map,这个就相当于是shared weights了,因为这个filter实际上被用了好多次.unshared weights filter就是由一组filters来产生一个feature map,一组指的就是一个filter窗口经过步长移动后,在新的位置上的其实是一个新的filter,所以一组的大小就是由窗口的大小和步长来确定的,它需要覆盖住整个输入(其实和之前的卷积filter没区别,只是这次要计算的参数就多了).</p>
<h2 id="ReLU_Nonlinearity">ReLU Nonlinearity</h2><p>这是一个新的激活函数,地位就和NN里的sigmoid函数一样,不过这个很特殊,从图上就能看出.</p>
<p><center><img src="/images/imagenet-cnn/cnn1.png" width="500px"></center><br>以前我们用sigmoid做激活函数,因为sigmoid的输出在[0, 1]之间,可以看做概率值.ReLU的输出是0到正无穷,可以用来建模实数输出.另外,ReLU的梯度不会消失,实际上它的梯度就是0或者1,这样整个网络收敛快了.ReLU具体<a href="http://www.quora.com/What-is-special-about-rectifier-neural-units-used-in-NN-learning" target="_blank" rel="external">参考这里</a>.</p>
<p><strong>Local Response Normalization</strong><br>这个东西就是对同一个输入(图片,或者图片卷积后的feature map)产生的好多feature maps做一个归一化,作为输出.具体地,就是在同一个空间位置(spatial position)(x, y)上,比如这个位置上如果有32个feature map,那么第7个feature map上的输出实际上考虑了从第5个到第9个feature map上同一个位置上的输出,对这些输出做了一个归一化.下面式子里的N指当前输出的总的feature map个数(对应着上层filters的个数),式子里的超参数k,alpha,beta都需要通过验证集调参得到.</p>
<p><center><img src="/images/imagenet-cnn/cnn2.png" width="500px"></center><br>文中说这种想法由真实神经元中的lateral inhibition现象而来,可以使不同的filter产生出的feature maps中较大激活的输出之间产生竞争(大概经过训练后会得到更好的特征吧).</p>
<p><strong>Overlapping Pooling</strong></p>
<p>这个和SPP里的池化窗口类似,即以前出的CNN池化窗口是不重合的,像这张图.这里的池化窗口也有一个步长,之间可以重叠(都到这一步了你怎么不直接用它来做不同维度输入得到相同大小输出的SPP呢你可是大牛啊).</p>
<p><center><img src="/images/cnn_2.gif" width="500px"></center></p>
<p><center><img src="/images/spp/spp6.png" width="500px"></center></p>
<h2 id="Overall_Architecture">Overall Architecture</h2><p>整个网络共8层,前5曾卷积网络,后3层全连网络,最后接一个1000路softmax分类器.这图分了上下两部分,代表两个GPU,各训练一半的filter和神经元.不过两个GPU会在第2,3层通信,作者说这是一个trick.这图里没有把池化和归一化操作画成图,池化只在第一层,第二层,第五层之后做了.</p>
<p><center><img src="/images/imagenet-cnn/cnn3.png" width="800px"></center><br>从图上看,输入是224x224x3,第一层是96个filter,两个GPU各接受48个,filter大小是11x11x3,所以每一个GPU,第一层的feature map大小是48x55x55(55是卷积的时候用了步长为4得出的,原图片作为输入的时候肯定补0了,这是cuda-convnet自己做的).然后这个图就看不懂了.输入经过卷积后每一个GPU的输出是48x55x55,然后是最大池化,应该是变成了48x27x27再输入到第二层网络(当然,还要normalization一下),但这个图上没把pooling层画出来,看不到这个48x27x27,到是有一个128x27x27,怎么回事?128不应该是再下一层的filter个数吗?这里只能先放着吧.</p>
<h2 id="Data_Augmentation">Data Augmentation</h2><p>首先做了好多裁剪和镜像.<br>还有就是对RGB做了处理.具体是对每一张图片上的每一个像素点,它对应了一个3维向量(RGB的3个值),那么对整个ImageNet,做由每一个像素点组成的矩阵的PCA,像素点组成的矩阵的协方差矩阵是3x3的,所以能计算出3个特征值和特征向量.然后改变以前的RGB三维向量,具体就是在以前每个像素点的RGB三维向量\([I_{xy}^R, I^G, I^B]\)(xy是像素坐标)上加这么个东西.</p>
<p><center><img src="/images/imagenet-cnn/cnn5.png" width="400px"></center><br>其中p1,p2,p3是特征向量,lambda是特征值,alpha是一个0均值,标准差0.1的高斯分布随机值.</p>
<h2 id="Dropout">Dropout</h2><p>怎么在训练一个模型的时候,能训练出多个模型,进而使用多个模型来预测,降低测试误差?Hinton想出的办法就是这个Dropout,把神经元的输出以0.5的概率置零,相当与扔掉了,被扔掉的神经元在前馈和反馈中都不起作用.所以每当有一个新输入时,由于Dropout的随机性,相当与在训练一个新的网络,但是这所有的网络是共享参数的.<br>这种策略减少了神经元之间的互相影响的一些现象,是的神经元不能依赖某一个特别的神经元是否出现,这样模型就能训练出更好的特性.<br>测试的时候就不dropout了,而是给每个神经元输出值乘以0.5,作为引入了dropout后预测分布的均值的估计.<br>作者在CNN网络之后的全连网络里引入了Dropout(原来那两个全连网络是干这事的).Dropout使迭代次数几乎翻倍才达到收敛,不过很好的减少了过拟合.</p>
<h2 id="讨论">讨论</h2><p>最后作者除了秀了一下AlexNet在各种数据库上的成绩,还贴了张挺惊人的图.</p>
<p><center><img src="/images/imagenet-cnn/cnn4.png" width="500px"></center><br>在1000路softmax分类器之前有4096个神经元输出,一张图片输入进网络走到这一步就变成了一个4096维的向量,第一列是5张测试图片,而其它几列则是整个数据集中的图片产生的4096维向量,与测试图片的向量欧式距离最近的6个.作者说,给这4096维的向量做个自动编码器映射成二维向量后,估计又能当图片检索来用了.</p>

      
    </div>
    <footer>
      
        
  
  <div class="categories">
    <a href="/categories/Whatyouknowabout/">Whatyouknowabout</a>
  </div>

        
  
  <div class="tags">
    <a href="/tags/CNN/">CNN</a>, <a href="/tags/Deep-Learning/">Deep Learning</a>, <a href="/tags/Imagenet/">Imagenet</a>, <a href="/tags/Paper/">Paper</a>
  </div>

        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
<!-- Duoshuo Comment BEGIN -->
	<div class="ds-thread"></div>
<script type="text/javascript">
var duoshuoQuery = {short_name:"sinb"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = 'http://static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		|| document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
<!-- Duoshuo Comment END -->
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="搜尋">
    <input type="hidden" name="q" value="site:sinb.github.io">
  </form>
</div>

  
<div class="widget tag">
  <h3 class="title">分類</h3>
  <ul class="entry">
  
    <li><a href="/categories/Artworks/">Artworks</a><small>8</small></li>
  
    <li><a href="/categories/Courseranote/">Courseranote</a><small>5</small></li>
  
    <li><a href="/categories/Life/">Life</a><small>7</small></li>
  
    <li><a href="/categories/Whatyouknowabout/">Whatyouknowabout</a><small>20</small></li>
  
  </ul>
</div>


  
<div class="widget tag">
  <h3 class="title">標籤</h3>
  <ul class="entry">
  
    <li><a href="/tags/8bit/">8bit</a><small>1</small></li>
  
    <li><a href="/tags/Algorithms/">Algorithms</a><small>4</small></li>
  
    <li><a href="/tags/CNN/">CNN</a><small>2</small></li>
  
    <li><a href="/tags/CS-Foundation/">CS Foundation</a><small>4</small></li>
  
    <li><a href="/tags/Convolution/">Convolution</a><small>1</small></li>
  
    <li><a href="/tags/Cosine/">Cosine</a><small>1</small></li>
  
    <li><a href="/tags/Coursera/">Coursera</a><small>4</small></li>
  
    <li><a href="/tags/DBN/">DBN</a><small>1</small></li>
  
    <li><a href="/tags/DCT/">DCT</a><small>1</small></li>
  
    <li><a href="/tags/DFT/">DFT</a><small>1</small></li>
  
    <li><a href="/tags/Deep-Learning/">Deep Learning</a><small>2</small></li>
  
    <li><a href="/tags/FC/">FC</a><small>2</small></li>
  
    <li><a href="/tags/HMM/">HMM</a><small>1</small></li>
  
    <li><a href="/tags/Imagenet/">Imagenet</a><small>1</small></li>
  
    <li><a href="/tags/MATLAB/">MATLAB</a><small>1</small></li>
  
    <li><a href="/tags/MNIST/">MNIST</a><small>1</small></li>
  
    <li><a href="/tags/NES/">NES</a><small>2</small></li>
  
    <li><a href="/tags/PCA/">PCA</a><small>1</small></li>
  
    <li><a href="/tags/Paper/">Paper</a><small>3</small></li>
  
    <li><a href="/tags/Processing/">Processing</a><small>5</small></li>
  
    <li><a href="/tags/Theano/">Theano</a><small>1</small></li>
  
    <li><a href="/tags/UFLDL/">UFLDL</a><small>7</small></li>
  
    <li><a href="/tags/Viterbi/">Viterbi</a><small>1</small></li>
  
    <li><a href="/tags/WeekEnd/">WeekEnd</a><small>1</small></li>
  
    <li><a href="/tags/fft/">fft</a><small>1</small></li>
  
    <li><a href="/tags/优先队列/">优先队列</a><small>0</small></li>
  
    <li><a href="/tags/分词/">分词</a><small>1</small></li>
  
    <li><a href="/tags/吉他/">吉他</a><small>1</small></li>
  
    <li><a href="/tags/喝酒/">喝酒</a><small>1</small></li>
  
    <li><a href="/tags/堆排序/">堆排序</a><small>0</small></li>
  
    <li><a href="/tags/排序/">排序</a><small>2</small></li>
  
    <li><a href="/tags/最小二乘/">最小二乘</a><small>1</small></li>
  
    <li><a href="/tags/最近/">最近</a><small>1</small></li>
  
    <li><a href="/tags/树/">树</a><small>1</small></li>
  
    <li><a href="/tags/梯度下降/">梯度下降</a><small>1</small></li>
  
    <li><a href="/tags/演出/">演出</a><small>1</small></li>
  
    <li><a href="/tags/爬虫/">爬虫</a><small>1</small></li>
  
    <li><a href="/tags/电影/">电影</a><small>2</small></li>
  
    <li><a href="/tags/超级玛丽/">超级玛丽</a><small>2</small></li>
  
  </ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2015 Amnesia
  
</div>
<div class="clearfix"></div></footer>
  <script src="http://ajax.useso.com/ajax/libs/jquery/1.7.2/jquery.min.js"></script>
<script src="/js/gallery.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>




</body>
</html>